### Setup
## Docker
​We have a Dockerfile that sets up this project. To use it, run the following command in the root directory
```
docker build . -t sw
docker run -it sw
```
In docker, run the following command to start the pipeline
```
cd src;
node crawl.js
```

## Local
Below are the installation tips:  
Installation tips for Ubuntu users:  

* npm: `apt install npm`
* Node.js: `npm i -g n; n 15.5.0`


​Installation tips for MacOS users:  

* npm: brew install npm
* Node.js: npm i -g n; n 15.5.0

In the project root directory, run:
`npm i`

### Running pipeline 

To **convert csv file to txt** file, just run `node parse.js`


To **run a full crawl**, first put your target website url to `data.txt` file, and run `node crawl.js`.   

Program will show you how many websites you crawled, and how many of them contains service worker.

SW's source code for each url is in `output` directory, and url is in `sw_url.txt` file.

To **analyze source code** of websites and find whether they use service worker or not, run `node request.js`. Change file name to the file that you want to analyze.

To **download source code** of service worker, in the `src` directory, run `chmod 777 download.sh`, and then run `./download.sh $(path)`, where $path is the argument that takes in path of the input file that you want to download. The output is in `sw_source_code` directory.

To **analyze if a service worker** `importScripts` **is importing a third-party script**, run `python analyze.py`.

To **open tab** for certain service worker files or website, run `python open_tab.py`.

