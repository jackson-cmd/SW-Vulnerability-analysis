const puppeteer = require('puppeteer');
var fs = require('fs');
const { parse } = require('csv-parse');
const lineByLine = require('n-readlines');
const https = require('https');
var number_sw = 0;
var number_dns_error = 0;

let sw = async (url) => {
  const browser = await puppeteer.launch({
    headless: true,
    args: [
      '--disable-gpu',
      '--disable-dev-shm-usage',
      '--disable-setuid-sandbox',
      '--no-sandbox',
    ],
  });
  try {
    const page = await browser.newPage();
    await page.setDefaultNavigationTimeout(30000);
    await page.goto(url);
    await console.log('Opened the URL:' + url);
    const swTarget = await browser.waitForTarget(
      (target) => {
        //console.log(target.url());
        return target.type() === 'service_worker';
      },
      { timeout: 10000 }
    );
    await console.log(`SW url is ${swTarget.url()}`);
    await number_sw++;
    await https.get(swTarget.url(), async (res) => {
      // Image will be stored at this pathW
      const path = '../output/' + url.slice(8) + '.txt';
      const filePath = fs.createWriteStream(path, { flags: 'w' });
      await res.pipe(filePath);
      await filePath.on('finish', async () => {
        await filePath.close();
        await console.log('Download Completed');
      });
    });
    await fs.appendFileSync('sw_url.txt', swTarget.url() + '\n');
    await console.log('Finish ' + url);
    await browser.close();
    return;
  } catch (err) {
    await console.log(`Don't have SW: ${err.message}`);
    if (err.message.search('net::ERR') >= 0) {
      await number_dns_error++;
    }
    await console.log('Finish ' + url);
    await browser.close();
    return null;
  }
};

async function pipeline_url(url) {
  await sw(url);
}

//pipeline to crawl websites
let pipe = async () => {
  try {
    // read contents of the file
    const data = fs.readFileSync('data.txt', 'UTF-8');

    // split the contents by new line
    const lines = data.split(/\r?\n/);

    // print all lines
    let idx = 0;
    for (let i of lines) {
      await console.log(`[Crawl for No.${idx} url]:${i}`);
      await idx++;
      await pipeline_url(i);
    }
    await console.log('Pipeline complete');
    await console.log(
      `Crawled ${idx} websites, found ${number_sw} SW, Can't access websites ${number_dns_error}`
    );
  } catch (err) {
    console.error(err);
  }
};
pipe();
